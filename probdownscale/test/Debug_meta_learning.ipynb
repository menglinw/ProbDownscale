{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import probdownscale\n",
    "reload(probdownscale.TaskExtractor)\n",
    "import probdownscale.MetaTrain as meta\n",
    "\n",
    "from probdownscale.TaskExtractor import TaskExtractor\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "file_path_g_05 = r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\G5NR_aerosol_variables_over_MiddleEast_daily_20050516-20060515.nc'\n",
    "file_path_g_06 =  r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\G5NR_aerosol_variables_over_MiddleEast_daily_20060516-20070515.nc'\n",
    "file_path_m = r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\MERRA2_aerosol_variables_over_MiddleEast_daily_20000516-20180515.nc'\n",
    "target_var = 'BCSMASS'\n",
    "\n",
    "# read data\n",
    "g05_data = nc.Dataset(file_path_g_05)\n",
    "g06_data = nc.Dataset(file_path_g_06)\n",
    "m_data_nc = nc.Dataset(file_path_m)\n",
    "\n",
    "# define lat&lon of MERRA, G5NR and mete\n",
    "M_lons = m_data_nc.variables['lon'][:]\n",
    "# self.M_lons = (M_lons-M_lons.mean())/M_lons.std()\n",
    "M_lats = m_data_nc.variables['lat'][:]\n",
    "# self.M_lats = (M_lats-M_lats.mean())/M_lats.std()\n",
    "G_lons = g05_data.variables['lon'][:]\n",
    "# self.G_lons = (G_lons-G_lons.mean())/G_lons.std()\n",
    "G_lats = g05_data.variables['lat'][:]\n",
    "\n",
    "# extract target data\n",
    "g_data = np.concatenate((g05_data.variables[target_var][:], g06_data.variables[target_var][:]), axis=0)\n",
    "m_data = m_data_nc.variables[target_var][5*365:7*365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [g_data, m_data]\n",
    "lats_lons = [G_lats, G_lons, M_lats, M_lons]\n",
    "task_dim = 10\n",
    "n_task = 10\n",
    "test_proportion = 0.3\n",
    "n_lag = 10\n",
    "taskextractor = TaskExtractor(data, lats_lons, task_dim, n_task, test_proportion, n_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_x_1, train_x_2, train_x_3], train_y, [test_x_1, test_x_2, test_x_3], test_y = taskextractor._get_random_task()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 10, 10)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 8, 9, 0], dtype=int32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last channel is the comonents\n",
    "alpha = np.random.rand(3,2,5)\n",
    "alpha = alpha/alpha.sum()\n",
    "mu = np.ones((3, 2, 5))\n",
    "test_md = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "        components_distribution=tfd.Exponential(\n",
    "        rate=mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float64, numpy=\n",
       "array([[-1., -1.],\n",
       "       [-1., -1.],\n",
       "       [-1., -1.]])>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_md.log_prob(np.ones((3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =  np.random.rand(3,3,3)\n",
    "#a = (a - a.min())/(a.max() - a.min())\n",
    "a = a/a.sum()\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnelu(input):\n",
    "    return tf.add(tf.constant(1, dtype=tf.float32), tf.nn.elu(input))\n",
    "\n",
    "components = 100\n",
    "no_parameters = 2\n",
    "def slice_parameter_vectors(parameter_vector):\n",
    "    return [parameter_vector[:, i * task_dim*task_dim*components:(i + 1) *task_dim*task_dim*components] for i in range(no_parameters)]\n",
    "\n",
    "def gnll_loss(y, parameter_vector):\n",
    "    alpha, mu = slice_parameter_vectors(parameter_vector)  # Unpack parameter vectors\n",
    "    #print(alpha.shape, mu.shape)\n",
    "    alpha1 = tf.reshape(alpha, (tf.shape(alpha)[0], task_dim, task_dim, components))\n",
    "    mu1 = tf.reshape(mu, (tf.shape(mu)[0], task_dim, task_dim, components))\n",
    "    #print(alpha1.shape, mu1.shape)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha1),\n",
    "        components_distribution=tfd.Exponential(\n",
    "        rate=mu1), allow_nan_stats=False\n",
    "    )\n",
    "\n",
    "    log_likelihood = gm.log_prob(y)  # Evaluate log-probability of y\n",
    "    \n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'nnelu': layers.Activation(nnelu)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_68 (InputLayer)          [(None, 10, 1, 10,   0           []                               \n",
      "                                10)]                                                              \n",
      "                                                                                                  \n",
      " input_69 (InputLayer)          [(None, 10, 10, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_70 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " conv_lstm2d_34 (ConvLSTM2D)    (None, 1, 9, 20)     4880        ['input_68[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 9, 9, 20)     100         ['input_69[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1)           4           ['input_70[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " flatten_45 (Flatten)           (None, 180)          0           ['conv_lstm2d_34[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_46 (Flatten)           (None, 1620)         0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " dense_69 (Dense)               (None, 30)           60          ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 1830)         0           ['flatten_45[0][0]',             \n",
      "                                                                  'flatten_46[0][0]',             \n",
      "                                                                  'dense_69[0][0]']               \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 10000)        18310000    ['concatenate_45[0][0]']         \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 10000)        18310000    ['concatenate_45[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 20000)        0           ['dense_70[0][0]',               \n",
      "                                                                  'dense_71[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 36,625,044\n",
      "Trainable params: 36,625,042\n",
      "Non-trainable params: 2\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MDN model\n",
    "# input dim (time, channel, rows, cols)\n",
    "input1 = layers.Input(shape=(n_lag, 1, task_dim, task_dim)) \n",
    "input2 = layers.Input(shape=(task_dim, task_dim, 1))\n",
    "input3 = layers.Input(shape=(1))\n",
    "\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='tanh', return_sequences=False)(input1)\n",
    "X = layers.Flatten()(X)\n",
    "\n",
    "X1 = layers.Conv2D(20, (2,2), activation='tanh')(input2)\n",
    "X1 = layers.Flatten()(X1)\n",
    "X2 = layers.BatchNormalization()(input3)\n",
    "X2 = layers.Dense(30, activation='relu')(X2)\n",
    "\n",
    "X = layers.Concatenate()([X, X1, X2])\n",
    "\n",
    "alphas = layers.Dense(components*task_dim*task_dim, activation=\"softmax\")(X)\n",
    "#alphas = layers.Reshape((task_dim, task_dim, components), name=\"alphas\")(alphas)\n",
    "mus = layers.Dense(components*task_dim*task_dim, activation=\"nnelu\")(X)\n",
    "#mus = layers.Reshape((task_dim, task_dim, components) ,name=\"mus\")(mus)\n",
    "#sigmas = layers.Dense(components, activation=\"nnelu\", name=\"sigmas\")(X)\n",
    "output = layers.Concatenate()([alphas, mus])\n",
    "MDN_model = Model([input1, input2, input3], output)\n",
    "MDN_model.compile(optimizer='adam', loss=gnll_loss)\n",
    "MDN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 7s 260ms/step - loss: -0.0502\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 4s 266ms/step - loss: -0.4720\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 4s 269ms/step - loss: -1.4893\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 4s 262ms/step - loss: -2.5068\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 4s 261ms/step - loss: -3.1417\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 4s 270ms/step - loss: -3.5134\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 4s 262ms/step - loss: -3.7662\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 4s 262ms/step - loss: -3.9584\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 4s 261ms/step - loss: -4.1177\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 4s 261ms/step - loss: -4.2559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee86e9f148>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDN_model.fit([train_x_1, train_x_2, train_x_3], train_y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Downscale_env",
   "language": "python",
   "name": "downscale_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
