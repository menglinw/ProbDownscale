{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import probdownscale\n",
    "reload(probdownscale.TaskExtractor)\n",
    "reload(probdownscale.MetaTrain)\n",
    "from probdownscale.MetaTrain import MetaSGD\n",
    "\n",
    "from probdownscale.TaskExtractor import TaskExtractor\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Debug TaskExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "file_path_g_05 = r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\G5NR_aerosol_variables_over_MiddleEast_daily_20050516-20060515.nc'\n",
    "file_path_g_06 =  r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\G5NR_aerosol_variables_over_MiddleEast_daily_20060516-20070515.nc'\n",
    "file_path_m = r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\MERRA2_aerosol_variables_over_MiddleEast_daily_20000516-20180515.nc'\n",
    "target_var = 'BCSMASS'\n",
    "\n",
    "# read data\n",
    "g05_data = nc.Dataset(file_path_g_05)\n",
    "g06_data = nc.Dataset(file_path_g_06)\n",
    "m_data_nc = nc.Dataset(file_path_m)\n",
    "\n",
    "# define lat&lon of MERRA, G5NR and mete\n",
    "M_lons = m_data_nc.variables['lon'][:]\n",
    "# self.M_lons = (M_lons-M_lons.mean())/M_lons.std()\n",
    "M_lats = m_data_nc.variables['lat'][:]\n",
    "# self.M_lats = (M_lats-M_lats.mean())/M_lats.std()\n",
    "G_lons = g05_data.variables['lon'][:]\n",
    "# self.G_lons = (G_lons-G_lons.mean())/G_lons.std()\n",
    "G_lats = g05_data.variables['lat'][:]\n",
    "\n",
    "# extract target data\n",
    "g_data = np.concatenate((g05_data.variables[target_var][:], g06_data.variables[target_var][:]), axis=0)\n",
    "m_data = m_data_nc.variables[target_var][5*365:7*365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [g_data, m_data]\n",
    "lats_lons = [G_lats, G_lons, M_lats, M_lons]\n",
    "task_dim = 10\n",
    "test_proportion = 0.3\n",
    "n_lag = 10\n",
    "taskextractor = TaskExtractor(data, lats_lons, task_dim, test_proportion, n_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = taskextractor.get_random_tasks(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y = taskextractor._get_random_task(is_random=False, record=False, lat_lon=(13.5625, 49.375))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last channel is the comonents\n",
    "alpha = np.random.rand(3,2,5)\n",
    "alpha = alpha/alpha.sum()\n",
    "mu = np.ones((3, 2, 5))\n",
    "test_md = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "        components_distribution=tfd.Exponential(\n",
    "        rate=mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float64, numpy=\n",
       "array([[-1., -1.],\n",
       "       [-1., -1.],\n",
       "       [-1., -1.]])>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_md.log_prob(np.ones((3,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =  np.random.rand(3,3,3)\n",
    "#a = (a - a.min())/(a.max() - a.min())\n",
    "a = a/a.sum()\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnelu(input):\n",
    "    return tf.add(tf.constant(1, dtype=tf.float32), tf.nn.elu(input))\n",
    "\n",
    "components = 100\n",
    "no_parameters = 2\n",
    "def slice_parameter_vectors(parameter_vector):\n",
    "    return [parameter_vector[:, i * task_dim*task_dim*components:(i + 1) *task_dim*task_dim*components] for i in range(no_parameters)]\n",
    "\n",
    "def gnll_loss(y, parameter_vector):\n",
    "    alpha, mu = slice_parameter_vectors(parameter_vector)  # Unpack parameter vectors\n",
    "    #print(alpha.shape, mu.shape)\n",
    "    alpha1 = tf.reshape(alpha, (tf.shape(alpha)[0], task_dim, task_dim, components))\n",
    "    mu1 = tf.reshape(mu, (tf.shape(mu)[0], task_dim, task_dim, components))\n",
    "    #print(alpha1.shape, mu1.shape)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha1),\n",
    "        components_distribution=tfd.Exponential(\n",
    "        rate=mu1), allow_nan_stats=False\n",
    "    )\n",
    "\n",
    "    log_likelihood = gm.log_prob(y)  # Evaluate log-probability of y\n",
    "    \n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'nnelu': layers.Activation(nnelu)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_30\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_68 (InputLayer)          [(None, 10, 1, 10,   0           []                               \n",
      "                                10)]                                                              \n",
      "                                                                                                  \n",
      " input_69 (InputLayer)          [(None, 10, 10, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " input_70 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " conv_lstm2d_34 (ConvLSTM2D)    (None, 1, 9, 20)     4880        ['input_68[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 9, 9, 20)     100         ['input_69[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1)           4           ['input_70[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " flatten_45 (Flatten)           (None, 180)          0           ['conv_lstm2d_34[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_46 (Flatten)           (None, 1620)         0           ['conv2d_14[0][0]']              \n",
      "                                                                                                  \n",
      " dense_69 (Dense)               (None, 30)           60          ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " concatenate_45 (Concatenate)   (None, 1830)         0           ['flatten_45[0][0]',             \n",
      "                                                                  'flatten_46[0][0]',             \n",
      "                                                                  'dense_69[0][0]']               \n",
      "                                                                                                  \n",
      " dense_70 (Dense)               (None, 10000)        18310000    ['concatenate_45[0][0]']         \n",
      "                                                                                                  \n",
      " dense_71 (Dense)               (None, 10000)        18310000    ['concatenate_45[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_46 (Concatenate)   (None, 20000)        0           ['dense_70[0][0]',               \n",
      "                                                                  'dense_71[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 36,625,044\n",
      "Trainable params: 36,625,042\n",
      "Non-trainable params: 2\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MDN model\n",
    "# input dim (time, channel, rows, cols)\n",
    "input1 = layers.Input(shape=(n_lag, 1, task_dim, task_dim)) \n",
    "input2 = layers.Input(shape=(task_dim, task_dim, 1))\n",
    "input3 = layers.Input(shape=(1))\n",
    "\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='tanh', return_sequences=False)(input1)\n",
    "X = layers.Flatten()(X)\n",
    "\n",
    "X1 = layers.Conv2D(20, (2,2), activation='tanh')(input2)\n",
    "X1 = layers.Flatten()(X1)\n",
    "X2 = layers.BatchNormalization()(input3)\n",
    "X2 = layers.Dense(30, activation='relu')(X2)\n",
    "\n",
    "X = layers.Concatenate()([X, X1, X2])\n",
    "\n",
    "alphas = layers.Dense(components*task_dim*task_dim, activation=\"softmax\")(X)\n",
    "#alphas = layers.Reshape((task_dim, task_dim, components), name=\"alphas\")(alphas)\n",
    "mus = layers.Dense(components*task_dim*task_dim, activation=\"nnelu\")(X)\n",
    "#mus = layers.Reshape((task_dim, task_dim, components) ,name=\"mus\")(mus)\n",
    "#sigmas = layers.Dense(components, activation=\"nnelu\", name=\"sigmas\")(X)\n",
    "output = layers.Concatenate()([alphas, mus])\n",
    "MDN_model = Model([input1, input2, input3], output)\n",
    "MDN_model.compile(optimizer='adam', loss=gnll_loss)\n",
    "MDN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 7s 260ms/step - loss: -0.0502\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 4s 266ms/step - loss: -0.4720\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 4s 269ms/step - loss: -1.4893\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 4s 262ms/step - loss: -2.5068\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 4s 261ms/step - loss: -3.1417\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 4s 270ms/step - loss: -3.5134\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 4s 262ms/step - loss: -3.7662\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 4s 262ms/step - loss: -3.9584\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 4s 261ms/step - loss: -4.1177\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 4s 261ms/step - loss: -4.2559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee86e9f148>"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MDN_model.fit([train_x_1, train_x_2, train_x_3], train_y, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Debug Meta Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define necessary tool functions\n",
    "components = 100\n",
    "no_parameters = 2\n",
    "\n",
    "def nnelu(input):\n",
    "    return tf.add(tf.constant(1, dtype=tf.float32), tf.nn.elu(input))\n",
    "\n",
    "def slice_parameter_vectors(parameter_vector):\n",
    "    return [parameter_vector[:, i * task_dim*task_dim*components:(i + 1) *task_dim*task_dim*components] for i in range(no_parameters)]\n",
    "\n",
    "def gnll_loss(y, parameter_vector):\n",
    "    alpha, mu = slice_parameter_vectors(parameter_vector)  # Unpack parameter vectors\n",
    "    #print(alpha.shape, mu.shape)\n",
    "    alpha1 = tf.reshape(alpha, (tf.shape(alpha)[0], task_dim, task_dim, components))\n",
    "    mu1 = tf.reshape(mu, (tf.shape(mu)[0], task_dim, task_dim, components))\n",
    "    #print(alpha1.shape, mu1.shape)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha1),\n",
    "        components_distribution=tfd.Exponential(\n",
    "        rate=mu1), allow_nan_stats=False\n",
    "    )\n",
    "\n",
    "    log_likelihood = gm.log_prob(y)  # Evaluate log-probability of y\n",
    "    \n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'nnelu': layers.Activation(nnelu)})\n",
    "\n",
    "# define MDN model\n",
    "# input dim (time, channel, rows, cols)\n",
    "input1 = layers.Input(shape=(n_lag, 1, task_dim, task_dim)) \n",
    "input2 = layers.Input(shape=(task_dim, task_dim, 1))\n",
    "input3 = layers.Input(shape=(1))\n",
    "\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='tanh', return_sequences=False)(input1)\n",
    "X = layers.Flatten()(X)\n",
    "\n",
    "X1 = layers.Conv2D(20, (2,2), activation='tanh')(input2)\n",
    "X1 = layers.Flatten()(X1)\n",
    "X2 = layers.BatchNormalization()(input3)\n",
    "X2 = layers.Dense(30, activation='relu')(X2)\n",
    "\n",
    "X = layers.Concatenate()([X, X1, X2])\n",
    "\n",
    "alphas = layers.Dense(components*task_dim*task_dim, activation=\"softmax\")(X)\n",
    "#alphas = layers.Reshape((task_dim, task_dim, components), name=\"alphas\")(alphas)\n",
    "mus = layers.Dense(components*task_dim*task_dim, activation=\"nnelu\")(X)\n",
    "#mus = layers.Reshape((task_dim, task_dim, components) ,name=\"mus\")(mus)\n",
    "#sigmas = layers.Dense(components, activation=\"nnelu\", name=\"sigmas\")(X)\n",
    "output = layers.Concatenate()([alphas, mus])\n",
    "MDN_model = Model([input1, input2, input3], output)\n",
    "\n",
    "# define TaskExtractor\n",
    "data = [g_data, m_data]\n",
    "lats_lons = [G_lats, G_lons, M_lats, M_lons]\n",
    "task_dim = 10\n",
    "test_proportion = 0.3\n",
    "n_lag = 10\n",
    "taskextractor = TaskExtractor(data, lats_lons, task_dim, test_proportion, n_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(probdownscale.MetaTrain)\n",
    "from probdownscale.MetaTrain import MetaSGD\n",
    "# define meta learner\n",
    "meta_step = 10\n",
    "meta_optimizer = tf.keras.optimizers.RMSprop(0.1)\n",
    "inner_step = 1\n",
    "inner_optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "meta_learner = MetaSGD(MDN_model, gnll_loss, meta_step, meta_optimizer, inner_step, inner_optimizer, taskextractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origional lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0123>\n",
      "updated lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.0123>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-4.17285>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_learner._train_on_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 10, 80)\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 2, 20, 80)\n",
      "<class 'numpy.ndarray'>\n",
      "(80,)\n",
      "<class 'numpy.ndarray'>\n",
      "(2, 2, 1, 20)\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 30)\n",
      "<class 'numpy.ndarray'>\n",
      "(30,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1830, 10000)\n",
      "<class 'numpy.ndarray'>\n",
      "(10000,)\n",
      "<class 'numpy.ndarray'>\n",
      "(1830, 10000)\n",
      "<class 'numpy.ndarray'>\n",
      "(10000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for t in MDN_model.get_weights():\n",
    "    print(t.shape)\n",
    "    print(type(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 10, 80)\n",
      "(1, 2, 20, 80)\n",
      "(80,)\n",
      "(2, 2, 1, 20)\n",
      "(20,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1, 30)\n",
      "(30,)\n",
      "(1830, 10000)\n",
      "(10000,)\n",
      "(1830, 10000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "for t in MDN_model.trainable_variables:\n",
    "    print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42265273,  0.06462992,  0.78516736,  1.25093588,  0.83372054,\n",
       "         0.12684125,  1.23409446,  2.26263361,  1.56729192,  2.31819973],\n",
       "       [ 1.93449083,  1.08668188,  1.85857859, -0.48187132,  0.83773894,\n",
       "        -0.0148779 ,  0.18204529,  1.47166016,  0.96289486,  1.22284512],\n",
       "       [ 0.30987924,  0.51762376,  1.64761627,  0.68419885,  1.3715202 ,\n",
       "         1.5960097 ,  0.38286023,  1.53599298,  0.92622567,  2.26660407],\n",
       "       [ 0.45589063,  1.61386188,  0.73829364,  2.3284075 ,  0.28444655,\n",
       "         0.69278491,  1.0866214 ,  1.92874451,  1.35150042,  2.27992891],\n",
       "       [ 1.9689489 ,  0.62789535,  1.60048962,  0.38896286,  1.93708888,\n",
       "         2.4220651 ,  2.02954596,  1.27148274,  0.48648999,  1.17837296],\n",
       "       [ 1.39271132,  2.66557933,  0.47243303,  1.23612533,  0.91220452,\n",
       "         0.77176904, -1.50397331,  2.02389278,  2.83962546,  1.29593856],\n",
       "       [ 1.72467709,  1.00786031,  1.8163368 ,  1.8785098 ,  0.48277863,\n",
       "         2.45905549,  0.27121147,  1.10766889,  0.40262847,  0.02851175],\n",
       "       [ 2.58600727,  1.01649965,  2.20511715,  2.09627359,  2.02103324,\n",
       "        -0.73096707,  1.27929186,  1.44564477, -0.81325882,  1.84219201],\n",
       "       [ 0.56283327,  0.00633099,  1.08538333,  0.68716082,  1.32430836,\n",
       "         0.02253704,  0.33561673,  1.11041942,  2.39296423,  2.77812864],\n",
       "       [ 1.73943429,  1.99800135, -1.22213615,  0.85224714,  3.00667332,\n",
       "         2.68735694,  0.46034997,  0.40478436, -1.11634477, -0.82828169]])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(1, 1, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Downscale_env",
   "language": "python",
   "name": "downscale_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
