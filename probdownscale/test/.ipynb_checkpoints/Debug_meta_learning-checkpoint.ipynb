{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import probdownscale\n",
    "reload(probdownscale.TaskExtractor)\n",
    "reload(probdownscale.MetaTrain)\n",
    "from probdownscale.MetaTrain import MetaSGD\n",
    "\n",
    "from probdownscale.TaskExtractor import TaskExtractor\n",
    "import math\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Debug TaskExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:18: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "C:\\Users\\96349\\anaconda3\\envs\\Downscale_env\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "file_path_g_05 = r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\G5NR_aerosol_variables_over_MiddleEast_daily_20050516-20060515.nc'\n",
    "file_path_g_06 =  r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\G5NR_aerosol_variables_over_MiddleEast_daily_20060516-20070515.nc'\n",
    "file_path_m = r'C:\\Users\\96349\\Documents\\Downscale_data\\MERRA2\\MERRA2_aerosol_variables_over_MiddleEast_daily_20000516-20180515.nc'\n",
    "target_var = 'BCSMASS'\n",
    "\n",
    "# read data\n",
    "g05_data = nc.Dataset(file_path_g_05)\n",
    "g06_data = nc.Dataset(file_path_g_06)\n",
    "m_data_nc = nc.Dataset(file_path_m)\n",
    "\n",
    "# define lat&lon of MERRA, G5NR and mete\n",
    "M_lons = m_data_nc.variables['lon'][:30]\n",
    "# self.M_lons = (M_lons-M_lons.mean())/M_lons.std()\n",
    "M_lats = m_data_nc.variables['lat'][:30]\n",
    "# self.M_lats = (M_lats-M_lats.mean())/M_lats.std()\n",
    "G_lons = g05_data.variables['lon'][:50]\n",
    "# self.G_lons = (G_lons-G_lons.mean())/G_lons.std()\n",
    "G_lats = g05_data.variables['lat'][:50]\n",
    "\n",
    "# extract target data\n",
    "g_data = np.concatenate((g05_data.variables[target_var][:, :50, :50], g06_data.variables[target_var][:, :50, :50]), axis=0)\n",
    "m_data = m_data_nc.variables[target_var][5*365:7*365, :30, :30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0617477851652782e-10"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1298117177926947e-07"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [g_data, m_data]\n",
    "lats_lons = [G_lats, G_lons, M_lats, M_lons]\n",
    "task_dim = 3\n",
    "test_proportion = 0.3\n",
    "n_lag = 10\n",
    "taskextractor = TaskExtractor(data, lats_lons, task_dim, test_proportion, n_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, location = taskextractor._get_random_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = taskextractor.get_grid_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, locations = taskextractor.get_random_tasks(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, locations = taskextractor.get_random_tasks(locations=locations[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(504, 3, 3)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10//3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last channel is the comonents\n",
    "alpha = np.random.rand(3,2,5)\n",
    "alpha = alpha/alpha.sum()\n",
    "mu = np.random.rand(3, 2, 5)\n",
    "mu = np.abs(mu)\n",
    "test_md = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha),\n",
    "        components_distribution=tfd.Gamma(concentration=mu, rate=mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float64, numpy=\n",
       "array([[nan, nan],\n",
       "       [nan, nan],\n",
       "       [nan, nan]])>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_like = test_md.log_prob(np.ones((3,2))*-0.001)\n",
    "#-tf.reduce_mean(log_like, axis=-1)\n",
    "log_like\n",
    "\n",
    "# when the Y is really small, log_prob return a positive log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float64, numpy=\n",
       "array([[0.15880819, 0.10569395],\n",
       "       [0.17997603, 0.15908541],\n",
       "       [0.20016422, 0.19627221]])>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_md.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float64, numpy=\n",
       "array([[-1.07151844, -1.96976935],\n",
       "       [-0.88054633, -1.06112697],\n",
       "       [-1.63395563, -0.41498473]])>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =  np.random.rand(3,3,3)\n",
    "#a = (a - a.min())/(a.max() - a.min())\n",
    "a = a/a.sum()\n",
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define necessary tool functions\n",
    "components = 5\n",
    "no_parameters = 2\n",
    "\n",
    "def nnelu(input):\n",
    "    return tf.add(tf.constant(1, dtype=tf.float32), tf.nn.elu(input))\n",
    "\n",
    "def slice_parameter_vectors(parameter_vector, no_parameters):\n",
    "    return [parameter_vector[:, i * task_dim*task_dim*components:(i + 1) *task_dim*task_dim*components] for i in range(no_parameters)]\n",
    "\n",
    "def gamma_loss(y, parameter_vector):\n",
    "    alpha, mu, sigma = slice_parameter_vectors(parameter_vector, 3)  # Unpack parameter vectors\n",
    "    #print(alpha.shape, mu.shape)\n",
    "    alpha1 = tf.reshape(alpha, (tf.shape(alpha)[0], task_dim, task_dim, components))\n",
    "    mu1 = tf.reshape(mu, (tf.shape(mu)[0], task_dim, task_dim, components))\n",
    "    sigma1 = tf.reshape(sigma,  (tf.shape(sigma)[0], task_dim, task_dim, components))\n",
    "    #print(alpha1.shape, mu1.shape)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha1),\n",
    "        components_distribution=tfd.Gamma(\n",
    "        concentration=mu1, rate=sigma1)\n",
    "    )\n",
    "\n",
    "    log_likelihood = gm.log_prob(y)  # Evaluate log-probability of y\n",
    "    #print(log_likelihood)\n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)\n",
    "\n",
    "def exponential_loss(y, parameter_vector):\n",
    "    alpha, mu = slice_parameter_vectors(parameter_vector, 2)  # Unpack parameter vectors\n",
    "    #print(alpha.shape, mu.shape)\n",
    "    alpha1 = tf.reshape(alpha, (tf.shape(alpha)[0], task_dim, task_dim, components))\n",
    "    mu1 = tf.reshape(mu, (tf.shape(mu)[0], task_dim, task_dim, components))\n",
    "    #print(alpha1.shape, mu1.shape)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha1),\n",
    "        components_distribution=tfd.Exponential(\n",
    "        rate=mu1)\n",
    "    )\n",
    "\n",
    "    log_likelihood = gm.log_prob(y)  # Evaluate log-probability of y\n",
    "    #print(log_likelihood)\n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)\n",
    "\n",
    "def gamma_mean_loss(y, parameter_vector):\n",
    "    alpha, mu, sigma = slice_parameter_vectors(parameter_vector, 3)  # Unpack parameter vectors\n",
    "    #print(alpha.shape, mu.shape)\n",
    "    alpha1 = tf.reshape(alpha, (tf.shape(alpha)[0], task_dim, task_dim, components))\n",
    "    mu1 = tf.reshape(mu, (tf.shape(mu)[0], task_dim, task_dim, components))\n",
    "    sigma1 = tf.reshape(sigma,  (tf.shape(sigma)[0], task_dim, task_dim, components))\n",
    "    #print(alpha1.shape, mu1.shape)\n",
    "    gm = tfd.MixtureSameFamily(\n",
    "        mixture_distribution=tfd.Categorical(probs=alpha1),\n",
    "        components_distribution=tfd.Gamma(\n",
    "        concentration=mu1, rate=sigma1)\n",
    "    )\n",
    "\n",
    "    log_likelihood = tf.subtract(gm.log_prob(), y)  # Evaluate log-probability of y\n",
    "    #print(log_likelihood)\n",
    "    return -tf.reduce_mean(log_likelihood, axis=-1)\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'nnelu': layers.Activation(nnelu)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 10, 1, 3, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv_lstm2d (ConvLSTM2D)       (None, 10, 1, 2, 20  3760        ['input_4[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_lstm2d_1 (ConvLSTM2D)     (None, 10, 1, 1, 20  6480        ['conv_lstm2d[1][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_lstm2d_2 (ConvLSTM2D)     (None, 1, 1, 20)     3280        ['conv_lstm2d_1[1][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 20)           0           ['conv_lstm2d_2[1][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          10752       ['flatten[1][0]']                \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 3, 3, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 512)         2048        ['dense[1][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 2, 2, 20)     100         ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 1)           4           ['input_6[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 300)          153900      ['batch_normalization_3[1][0]']  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 80)           0           ['conv2d[1][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 30)           60          ['batch_normalization_4[1][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 410)          0           ['dense_1[1][0]',                \n",
      "                                                                  'flatten_1[1][0]',              \n",
      "                                                                  'dense_2[1][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          52608       ['concatenate[1][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 128)         512         ['dense_3[1][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 128)          16512       ['batch_normalization_5[1][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 128)         512         ['dense_4[1][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 128)          16512       ['batch_normalization_6[1][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 128)         512         ['dense_5[1][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 45)           5805        ['batch_normalization_7[1][0]']  \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 45)           5805        ['batch_normalization_7[1][0]']  \n",
      "                                                                                                  \n",
      " sigmas (Dense)                 (None, 45)           5805        ['batch_normalization_7[1][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 135)          0           ['dense_6[1][0]',                \n",
      "                                                                  'dense_7[1][0]',                \n",
      "                                                                  'sigmas[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 284,967\n",
      "Trainable params: 283,173\n",
      "Non-trainable params: 1,794\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define MDN Gamma model\n",
    "# input dim (time, channel, rows, cols)\n",
    "input1 = layers.Input(shape=(n_lag, 1, task_dim, task_dim)) \n",
    "input1 = layers.BatchNormalization()(input1)\n",
    "input2 = layers.Input(shape=(task_dim, task_dim, 1))\n",
    "input2 = layers.BatchNormalization()(input2)\n",
    "input3 = layers.Input(shape=(1))\n",
    "input3 = layers.BatchNormalization()(input3)\n",
    "\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='tanh', return_sequences=True)(input1)\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='relu', return_sequences=True)(X)\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,1), activation='relu')(X)\n",
    "X = layers.Flatten()(X)\n",
    "X = layers.Dense(512, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(300, activation='relu')(X)\n",
    "\n",
    "X1 = layers.Conv2D(20, (2,2), activation='tanh')(input2)\n",
    "X1 = layers.Flatten()(X1)\n",
    "X2 = layers.BatchNormalization()(input3)\n",
    "X2 = layers.Dense(30, activation='relu')(X2)\n",
    "\n",
    "X = layers.Concatenate()([X, X1, X2])\n",
    "X = layers.Dense(128, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(128, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(128, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "alphas = layers.Dense(components*task_dim*task_dim, activation=\"softmax\")(X)\n",
    "#alphas = layers.Reshape((task_dim, task_dim, components), name=\"alphas\")(alphas)\n",
    "mus = layers.Dense(components*task_dim*task_dim, activation='nnelu')(X)\n",
    "#mus = layers.Reshape((task_dim, task_dim, components) ,name=\"mus\")(mus)\n",
    "sigmas = layers.Dense(components*task_dim*task_dim, activation=\"nnelu\", name=\"sigmas\")(X)\n",
    "output = layers.Concatenate()([alphas, mus, sigmas])\n",
    "MDN_model_ref_gamma = Model([input1, input2, input3], output)\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "MDN_model_ref_gamma.compile(optimizer=optimizer, loss=gamma_loss)\n",
    "MDN_model_ref_gamma.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 10, 1, 3, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv_lstm2d_3 (ConvLSTM2D)     (None, 10, 1, 2, 20  3760        ['input_10[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_lstm2d_4 (ConvLSTM2D)     (None, 10, 1, 1, 20  6480        ['conv_lstm2d_3[1][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_lstm2d_5 (ConvLSTM2D)     (None, 1, 1, 20)     3280        ['conv_lstm2d_4[1][0]']          \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 20)           0           ['conv_lstm2d_5[1][0]']          \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 512)          10752       ['flatten_2[1][0]']              \n",
      "                                                                                                  \n",
      " input_11 (InputLayer)          [(None, 3, 3, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 512)         2048        ['dense_8[1][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 2, 2, 20)     100         ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 1)           4           ['input_12[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 300)          153900      ['batch_normalization_11[1][0]'] \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 80)           0           ['conv2d_1[1][0]']               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 30)           60          ['batch_normalization_12[1][0]'] \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 410)          0           ['dense_9[1][0]',                \n",
      "                                                                  'flatten_3[1][0]',              \n",
      "                                                                  'dense_10[1][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 128)          52608       ['concatenate_2[1][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 128)         512         ['dense_11[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 128)          16512       ['batch_normalization_13[1][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 128)         512         ['dense_12[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 128)          16512       ['batch_normalization_14[1][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 128)         512         ['dense_13[1][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 45)           5805        ['batch_normalization_15[1][0]'] \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 45)           5805        ['batch_normalization_15[1][0]'] \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 90)           0           ['dense_14[1][0]',               \n",
      "                                                                  'dense_15[1][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 279,162\n",
      "Trainable params: 277,368\n",
      "Non-trainable params: 1,794\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define MDN Exponentialmodel\n",
    "# input dim (time, channel, rows, cols)\n",
    "input1 = layers.Input(shape=(n_lag, 1, task_dim, task_dim)) \n",
    "input1 = layers.BatchNormalization()(input1)\n",
    "input2 = layers.Input(shape=(task_dim, task_dim, 1))\n",
    "input2 = layers.BatchNormalization()(input2)\n",
    "input3 = layers.Input(shape=(1))\n",
    "input3 = layers.BatchNormalization()(input3)\n",
    "\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='tanh', return_sequences=True)(input1)\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='relu', return_sequences=True)(X)\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,1), activation='relu')(X)\n",
    "X = layers.Flatten()(X)\n",
    "X = layers.Dense(512, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(300, activation='relu')(X)\n",
    "\n",
    "X1 = layers.Conv2D(20, (2,2), activation='tanh')(input2)\n",
    "X1 = layers.Flatten()(X1)\n",
    "X2 = layers.BatchNormalization()(input3)\n",
    "X2 = layers.Dense(30, activation='relu')(X2)\n",
    "\n",
    "X = layers.Concatenate()([X, X1, X2])\n",
    "X = layers.Dense(128, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(128, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(128, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "alphas = layers.Dense(components*task_dim*task_dim, activation=\"softmax\")(X)\n",
    "#alphas = layers.Reshape((task_dim, task_dim, components), name=\"alphas\")(alphas)\n",
    "mus = layers.Dense(components*task_dim*task_dim, activation='nnelu')(X)\n",
    "#mus = layers.Reshape((task_dim, task_dim, components) ,name=\"mus\")(mus)\n",
    "output = layers.Concatenate()([alphas, mus])\n",
    "MDN_model_ref_exponential = Model([input1, input2, input3], output)\n",
    "optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "MDN_model_ref_exponential.compile(optimizer=optimizer, loss=exponential_loss)\n",
    "MDN_model_ref_exponential.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "16/16 [==============================] - 10s 81ms/step - loss: 56.2201 - val_loss: 135.8587\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 41.6321 - val_loss: 138.3188\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 33.1582 - val_loss: 134.8584\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 27.2827 - val_loss: 130.5396\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 24.7253 - val_loss: 124.8872\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 20.6684 - val_loss: 119.5751\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 19.8310 - val_loss: 114.7926\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 17.9590 - val_loss: 110.1815\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 16.0594 - val_loss: 105.6263\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 15.7116 - val_loss: 101.0656\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 14.5754 - val_loss: 95.9548\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 14.0412 - val_loss: 90.2568\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 13.3619 - val_loss: 84.6878\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 12.3603 - val_loss: 78.9296\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 11.5565 - val_loss: 73.5455\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 11.8376 - val_loss: 68.7453\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 10.6252 - val_loss: 62.4855\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 10.7297 - val_loss: 54.4772\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 1s 31ms/step - loss: 10.3010 - val_loss: 48.5204\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 9.7382 - val_loss: 44.8627\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 9.6855 - val_loss: 38.5914\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 9.2569 - val_loss: 32.4864\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 9.2355 - val_loss: 28.0845\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 8.8802 - val_loss: 24.8006\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 8.8671 - val_loss: 21.8280\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 8.5612 - val_loss: 19.1730\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 8.2528 - val_loss: 16.9849\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 8.1808 - val_loss: 15.4327\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 8.3386 - val_loss: 13.9857\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 8.2410 - val_loss: 12.8405\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 8.0266 - val_loss: 11.7376\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 7.7682 - val_loss: 10.8588\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.8497 - val_loss: 10.1208\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.5400 - val_loss: 9.4620\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.4530 - val_loss: 8.9114\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 7.6787 - val_loss: 8.6331\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 7.5143 - val_loss: 8.1201\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 7.3649 - val_loss: 7.9591\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.4707 - val_loss: 7.7994\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.2380 - val_loss: 7.6711\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.2106 - val_loss: 7.6305\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 7.1711 - val_loss: 7.4848\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.9800 - val_loss: 7.3511\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.3789 - val_loss: 7.2740\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.1168 - val_loss: 7.2262\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.0325 - val_loss: 7.1714\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 7.0500 - val_loss: 7.1100\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.9010 - val_loss: 6.9884\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.9730 - val_loss: 7.0205\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.8699 - val_loss: 6.9865\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.8084 - val_loss: 6.9110\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.9065 - val_loss: 6.9108\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.7323 - val_loss: 6.8972\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.7318 - val_loss: 6.8622\n",
      "Epoch 55/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.7619 - val_loss: 6.7553\n",
      "Epoch 56/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.8056 - val_loss: 6.7256\n",
      "Epoch 57/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.7713 - val_loss: 6.7407\n",
      "Epoch 58/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.8524 - val_loss: 6.7348\n",
      "Epoch 59/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.7240 - val_loss: 6.7212\n",
      "Epoch 60/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.7854 - val_loss: 6.7035\n",
      "Epoch 61/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.6657 - val_loss: 6.7486\n",
      "Epoch 62/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.6694 - val_loss: 6.7451\n",
      "Epoch 63/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.6203 - val_loss: 6.6813\n",
      "Epoch 64/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.6853 - val_loss: 6.6332\n",
      "Epoch 65/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.7671 - val_loss: 6.6047\n",
      "Epoch 66/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.7042 - val_loss: 6.5698\n",
      "Epoch 67/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.5780 - val_loss: 6.5885\n",
      "Epoch 68/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.6025 - val_loss: 6.5557\n",
      "Epoch 69/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.5282 - val_loss: 6.5456\n",
      "Epoch 70/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4650 - val_loss: 6.5054\n",
      "Epoch 71/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.6453 - val_loss: 6.4756\n",
      "Epoch 72/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4721 - val_loss: 6.4841\n",
      "Epoch 73/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.5132 - val_loss: 6.4708\n",
      "Epoch 74/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4941 - val_loss: 6.4847\n",
      "Epoch 75/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.5614 - val_loss: 6.5092\n",
      "Epoch 76/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.5146 - val_loss: 6.4957\n",
      "Epoch 77/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.5756 - val_loss: 6.4636\n",
      "Epoch 78/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.5114 - val_loss: 6.4601\n",
      "Epoch 79/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.5678 - val_loss: 6.4708\n",
      "Epoch 80/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4952 - val_loss: 6.4747\n",
      "Epoch 81/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4792 - val_loss: 6.4559\n",
      "Epoch 82/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.5009 - val_loss: 6.4502\n",
      "Epoch 83/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4111 - val_loss: 6.4411\n",
      "Epoch 84/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.6179 - val_loss: 6.4531\n",
      "Epoch 85/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4715 - val_loss: 6.5851\n",
      "Epoch 86/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.3955 - val_loss: 6.5710\n",
      "Epoch 87/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4381 - val_loss: 6.5003\n",
      "Epoch 88/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4206 - val_loss: 6.4611\n",
      "Epoch 89/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.3888 - val_loss: 6.4369\n",
      "Epoch 90/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.4381 - val_loss: 6.4700\n",
      "Epoch 91/300\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.3851 - val_loss: 6.4377\n",
      "Epoch 92/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4314 - val_loss: 6.4291\n",
      "Epoch 93/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4052 - val_loss: 6.3844\n",
      "Epoch 94/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4026 - val_loss: 6.3885\n",
      "Epoch 95/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3885 - val_loss: 6.3720\n",
      "Epoch 96/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3473 - val_loss: 6.3531\n",
      "Epoch 97/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4364 - val_loss: 6.3691\n",
      "Epoch 98/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4222 - val_loss: 6.3598\n",
      "Epoch 99/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4115 - val_loss: 6.3629\n",
      "Epoch 100/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4369 - val_loss: 6.3688\n",
      "Epoch 101/300\n",
      "16/16 [==============================] - 1s 32ms/step - loss: 6.3909 - val_loss: 6.3684\n",
      "Epoch 102/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.5229 - val_loss: 6.3671\n",
      "Epoch 103/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4798 - val_loss: 6.3732\n",
      "Epoch 104/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3493 - val_loss: 6.3901\n",
      "Epoch 105/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3282 - val_loss: 6.3606\n",
      "Epoch 106/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3549 - val_loss: 6.3836\n",
      "Epoch 107/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4336 - val_loss: 6.3786\n",
      "Epoch 108/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3112 - val_loss: 6.3776\n",
      "Epoch 109/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3447 - val_loss: 6.3888\n",
      "Epoch 110/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3621 - val_loss: 6.4274\n",
      "Epoch 111/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3034 - val_loss: 6.4124\n",
      "Epoch 112/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2900 - val_loss: 6.3907\n",
      "Epoch 113/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3345 - val_loss: 6.3862\n",
      "Epoch 114/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3415 - val_loss: 6.3483\n",
      "Epoch 115/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4114 - val_loss: 6.3205\n",
      "Epoch 116/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3495 - val_loss: 6.3416\n",
      "Epoch 117/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3363 - val_loss: 6.3244\n",
      "Epoch 118/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3069 - val_loss: 6.3072\n",
      "Epoch 119/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2820 - val_loss: 6.3197\n",
      "Epoch 120/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2988 - val_loss: 6.3117\n",
      "Epoch 121/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3233 - val_loss: 6.3144\n",
      "Epoch 122/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3441 - val_loss: 6.3562\n",
      "Epoch 123/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2986 - val_loss: 6.3388\n",
      "Epoch 124/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3252 - val_loss: 6.3347\n",
      "Epoch 125/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.4326 - val_loss: 6.3159\n",
      "Epoch 126/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2800 - val_loss: 6.3398\n",
      "Epoch 127/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3146 - val_loss: 6.3133\n",
      "Epoch 128/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2917 - val_loss: 6.3244\n",
      "Epoch 129/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3699 - val_loss: 6.3752\n",
      "Epoch 130/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3263 - val_loss: 6.4076\n",
      "Epoch 131/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2490 - val_loss: 6.3807\n",
      "Epoch 132/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3048 - val_loss: 6.3727\n",
      "Epoch 133/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2947 - val_loss: 6.3928\n",
      "Epoch 134/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2767 - val_loss: 6.4203\n",
      "Epoch 135/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3049 - val_loss: 6.4277\n",
      "Epoch 136/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3296 - val_loss: 6.4198\n",
      "Epoch 137/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2926 - val_loss: 6.3462\n",
      "Epoch 138/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2546 - val_loss: 6.3412\n",
      "Epoch 139/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2950 - val_loss: 6.4114\n",
      "Epoch 140/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3292 - val_loss: 6.4229\n",
      "Epoch 141/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2523 - val_loss: 6.3766\n",
      "Epoch 142/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2358 - val_loss: 6.3318\n",
      "Epoch 143/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2719 - val_loss: 6.3448\n",
      "Epoch 144/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.3463 - val_loss: 6.3276\n",
      "Epoch 145/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2875 - val_loss: 6.3422\n",
      "Epoch 146/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2528 - val_loss: 6.4148\n",
      "Epoch 147/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3461 - val_loss: 6.3815\n",
      "Epoch 148/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2728 - val_loss: 6.3102\n",
      "Epoch 149/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.3341 - val_loss: 6.3554\n",
      "Epoch 150/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2651 - val_loss: 6.3352\n",
      "Epoch 151/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.3738 - val_loss: 6.3578\n",
      "Epoch 152/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2563 - val_loss: 6.3285\n",
      "Epoch 153/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2468 - val_loss: 6.3508\n",
      "Epoch 154/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2378 - val_loss: 6.3482\n",
      "Epoch 155/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2334 - val_loss: 6.3523\n",
      "Epoch 156/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2398 - val_loss: 6.4031\n",
      "Epoch 157/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2697 - val_loss: 6.3896\n",
      "Epoch 158/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2158 - val_loss: 6.3257\n",
      "Epoch 159/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2706 - val_loss: 6.3233\n",
      "Epoch 160/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2530 - val_loss: 6.4878\n",
      "Epoch 161/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2197 - val_loss: 6.4536\n",
      "Epoch 162/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2282 - val_loss: 6.3859\n",
      "Epoch 163/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2397 - val_loss: 6.3778\n",
      "Epoch 164/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2713 - val_loss: 6.3370\n",
      "Epoch 165/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2269 - val_loss: 6.3143\n",
      "Epoch 166/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2480 - val_loss: 6.4052\n",
      "Epoch 167/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2274 - val_loss: 6.4017\n",
      "Epoch 168/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2307 - val_loss: 6.3669\n",
      "Epoch 169/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2339 - val_loss: 6.3881\n",
      "Epoch 170/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2566 - val_loss: 6.4405\n",
      "Epoch 171/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2498 - val_loss: 6.4112\n",
      "Epoch 172/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2766 - val_loss: 6.3721\n",
      "Epoch 173/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2122 - val_loss: 6.3358\n",
      "Epoch 174/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2393 - val_loss: 6.3322\n",
      "Epoch 175/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2426 - val_loss: 6.3237\n",
      "Epoch 176/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2832 - val_loss: 6.3245\n",
      "Epoch 177/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2335 - val_loss: 6.4134\n",
      "Epoch 178/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2545 - val_loss: 6.4636\n",
      "Epoch 179/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2330 - val_loss: 6.4262\n",
      "Epoch 180/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.1704 - val_loss: 6.4638\n",
      "Epoch 181/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2558 - val_loss: 6.3533\n",
      "Epoch 182/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2299 - val_loss: 6.3213\n",
      "Epoch 183/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2198 - val_loss: 6.3137\n",
      "Epoch 184/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2432 - val_loss: 6.2902\n",
      "Epoch 185/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.2055 - val_loss: 6.3086\n",
      "Epoch 186/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2340 - val_loss: 6.3015\n",
      "Epoch 187/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2341 - val_loss: 6.3111\n",
      "Epoch 188/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.2660 - val_loss: 6.3230\n",
      "Epoch 189/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2306 - val_loss: 6.3672\n",
      "Epoch 190/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2096 - val_loss: 6.3834\n",
      "Epoch 191/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1811 - val_loss: 6.3786\n",
      "Epoch 192/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2117 - val_loss: 6.4161\n",
      "Epoch 193/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1986 - val_loss: 6.4301\n",
      "Epoch 194/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2449 - val_loss: 6.5151\n",
      "Epoch 195/300\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 6.1858 - val_loss: 6.4720\n",
      "Epoch 196/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2002 - val_loss: 6.3867\n",
      "Epoch 197/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2007 - val_loss: 6.3795\n",
      "Epoch 198/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2517 - val_loss: 6.3327\n",
      "Epoch 199/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2440 - val_loss: 6.3042\n",
      "Epoch 200/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1917 - val_loss: 6.3006\n",
      "Epoch 201/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1981 - val_loss: 6.3212\n",
      "Epoch 202/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.2035 - val_loss: 6.3062\n",
      "Epoch 203/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2031 - val_loss: 6.3677\n",
      "Epoch 204/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1959 - val_loss: 6.4184\n",
      "Epoch 205/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1791 - val_loss: 6.3995\n",
      "Epoch 206/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1950 - val_loss: 6.3191\n",
      "Epoch 207/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2036 - val_loss: 6.3265\n",
      "Epoch 208/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2036 - val_loss: 6.3119\n",
      "Epoch 209/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1938 - val_loss: 6.3060\n",
      "Epoch 210/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1860 - val_loss: 6.3130\n",
      "Epoch 211/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1501 - val_loss: 6.3225\n",
      "Epoch 212/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1857 - val_loss: 6.3537\n",
      "Epoch 213/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2099 - val_loss: 6.3500\n",
      "Epoch 214/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2107 - val_loss: 6.3552\n",
      "Epoch 215/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1715 - val_loss: 6.4219\n",
      "Epoch 216/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1883 - val_loss: 6.4586\n",
      "Epoch 217/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1682 - val_loss: 6.3195\n",
      "Epoch 218/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1708 - val_loss: 6.3553\n",
      "Epoch 219/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1725 - val_loss: 6.3601\n",
      "Epoch 220/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2560 - val_loss: 6.3151\n",
      "Epoch 221/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2014 - val_loss: 6.3946\n",
      "Epoch 222/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1778 - val_loss: 6.3857\n",
      "Epoch 223/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1758 - val_loss: 6.3649\n",
      "Epoch 224/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1927 - val_loss: 6.4402\n",
      "Epoch 225/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1911 - val_loss: 6.4920\n",
      "Epoch 226/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2154 - val_loss: 6.5133\n",
      "Epoch 227/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1709 - val_loss: 6.4146\n",
      "Epoch 228/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2450 - val_loss: 6.3733\n",
      "Epoch 229/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2217 - val_loss: 6.3618\n",
      "Epoch 230/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1683 - val_loss: 6.3837\n",
      "Epoch 231/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1631 - val_loss: 6.3624\n",
      "Epoch 232/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1803 - val_loss: 6.3888\n",
      "Epoch 233/300\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 6.1762 - val_loss: 6.4504\n",
      "Epoch 234/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1632 - val_loss: 6.3771\n",
      "Epoch 235/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1451 - val_loss: 6.3313\n",
      "Epoch 236/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1708 - val_loss: 6.3456\n",
      "Epoch 237/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1833 - val_loss: 6.3469\n",
      "Epoch 238/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2231 - val_loss: 6.4107\n",
      "Epoch 239/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1605 - val_loss: 6.3557\n",
      "Epoch 240/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2095 - val_loss: 6.3825\n",
      "Epoch 241/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1866 - val_loss: 6.4260\n",
      "Epoch 242/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1463 - val_loss: 6.4403\n",
      "Epoch 243/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1566 - val_loss: 6.4485\n",
      "Epoch 244/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1672 - val_loss: 6.4185\n",
      "Epoch 245/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1787 - val_loss: 6.3841\n",
      "Epoch 246/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1577 - val_loss: 6.3595\n",
      "Epoch 247/300\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 6.2039 - val_loss: 6.3597\n",
      "Epoch 248/300\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 6.1608 - val_loss: 6.4403\n",
      "Epoch 249/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.2083 - val_loss: 6.4034\n",
      "Epoch 250/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1690 - val_loss: 6.3683\n",
      "Epoch 251/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1377 - val_loss: 6.3822\n",
      "Epoch 252/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1975 - val_loss: 6.3698\n",
      "Epoch 253/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1492 - val_loss: 6.3637\n",
      "Epoch 254/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1713 - val_loss: 6.3454\n",
      "Epoch 255/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1512 - val_loss: 6.3154\n",
      "Epoch 256/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1614 - val_loss: 6.2895\n",
      "Epoch 257/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1560 - val_loss: 6.3225\n",
      "Epoch 258/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1524 - val_loss: 6.3331\n",
      "Epoch 259/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1551 - val_loss: 6.4022\n",
      "Epoch 260/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2001 - val_loss: 6.3203\n",
      "Epoch 261/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1424 - val_loss: 6.3259\n",
      "Epoch 262/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1667 - val_loss: 6.3483\n",
      "Epoch 263/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1607 - val_loss: 6.3677\n",
      "Epoch 264/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1419 - val_loss: 6.3592\n",
      "Epoch 265/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1421 - val_loss: 6.3800\n",
      "Epoch 266/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1539 - val_loss: 6.3158\n",
      "Epoch 267/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.2290 - val_loss: 6.2863\n",
      "Epoch 268/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1673 - val_loss: 6.3146\n",
      "Epoch 269/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1240 - val_loss: 6.3137\n",
      "Epoch 270/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1639 - val_loss: 6.2740\n",
      "Epoch 271/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1465 - val_loss: 6.2828\n",
      "Epoch 272/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1943 - val_loss: 6.3352\n",
      "Epoch 273/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1401 - val_loss: 6.3279\n",
      "Epoch 274/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1560 - val_loss: 6.3854\n",
      "Epoch 275/300\n",
      "16/16 [==============================] - 0s 29ms/step - loss: 6.1621 - val_loss: 6.3752\n",
      "Epoch 276/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.2165 - val_loss: 6.3782\n",
      "Epoch 277/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1594 - val_loss: 6.6083\n",
      "Epoch 278/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1519 - val_loss: 6.6692\n",
      "Epoch 279/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1399 - val_loss: 6.7561\n",
      "Epoch 280/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1339 - val_loss: 6.5803\n",
      "Epoch 281/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1296 - val_loss: 6.4789\n",
      "Epoch 282/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1284 - val_loss: 6.4182\n",
      "Epoch 283/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1510 - val_loss: 6.3902\n",
      "Epoch 284/300\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 6.1352 - val_loss: 6.4114\n",
      "Epoch 285/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1239 - val_loss: 6.4028\n",
      "Epoch 286/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1245 - val_loss: 6.4104\n",
      "Epoch 287/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1399 - val_loss: 6.3970\n",
      "Epoch 288/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1420 - val_loss: 6.4273\n",
      "Epoch 289/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1425 - val_loss: 6.3680\n",
      "Epoch 290/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1556 - val_loss: 6.4072\n",
      "Epoch 291/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1438 - val_loss: 6.3796\n",
      "Epoch 292/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1358 - val_loss: 6.3923\n",
      "Epoch 293/300\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 6.1426 - val_loss: 6.4056\n",
      "Epoch 294/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1380 - val_loss: 6.4341\n",
      "Epoch 295/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1662 - val_loss: 6.4243\n",
      "Epoch 296/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1655 - val_loss: 6.4503\n",
      "Epoch 297/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1421 - val_loss: 6.4364\n",
      "Epoch 298/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1549 - val_loss: 6.4592\n",
      "Epoch 299/300\n",
      "16/16 [==============================] - 0s 31ms/step - loss: 6.1508 - val_loss: 6.3681\n",
      "Epoch 300/300\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 6.1917 - val_loss: 6.3136\n"
     ]
    }
   ],
   "source": [
    "history = MDN_model_ref_exponential.fit(train_x, train_y, epochs=300, validation_data=[test_x, test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2658940a908>]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Gamma Mixture Reference Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Exponential Mixture Reference Model')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Exponential Mixture Reference Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Debug Meta Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define necessary tool functions\n",
    "components = 5\n",
    "no_parameters = 3\n",
    "data = [g_data, m_data]\n",
    "lats_lons = [G_lats, G_lons, M_lats, M_lons]\n",
    "task_dim = 3\n",
    "test_proportion = 0.5\n",
    "n_lag = 10\n",
    "\n",
    "# define MDN model\n",
    "# input dim (time, channel, rows, cols)\n",
    "input1 = layers.Input(shape=(n_lag, 1, task_dim, task_dim)) \n",
    "input1 = layers.BatchNormalization()(input1)\n",
    "input2 = layers.Input(shape=(task_dim, task_dim, 1))\n",
    "input2 = layers.BatchNormalization()(input2)\n",
    "input3 = layers.Input(shape=(1))\n",
    "input3 = layers.BatchNormalization()(input3)\n",
    "\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='tanh', return_sequences=True)(input1)\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,2), activation='relu', return_sequences=True)(X)\n",
    "X = layers.ConvLSTM2D(filters=20, kernel_size=(1,1), activation='relu')(X)\n",
    "X = layers.Flatten()(X)\n",
    "X = layers.Dense(512, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(300, activation='relu')(X)\n",
    "\n",
    "X1 = layers.Conv2D(20, (2,2), activation='tanh')(input2)\n",
    "X1 = layers.Flatten()(X1)\n",
    "X2 = layers.BatchNormalization()(input3)\n",
    "X2 = layers.Dense(30, activation='relu')(X2)\n",
    "\n",
    "X = layers.Concatenate()([X, X1, X2])\n",
    "X = layers.Dense(128, activation='tanh')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(128, activation='relu')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "X = layers.Dense(128, activation='tanh')(X)\n",
    "X = layers.BatchNormalization()(X)\n",
    "alphas = layers.Dense(components*task_dim*task_dim, activation=\"softmax\")(X)\n",
    "#alphas = layers.Reshape((task_dim, task_dim, components), name=\"alphas\")(alphas)\n",
    "mus = layers.Dense(components*task_dim*task_dim, activation='nnelu')(X)\n",
    "#mus = layers.Reshape((task_dim, task_dim, components) ,name=\"mus\")(mus)\n",
    "sigmas = layers.Dense(components*task_dim*task_dim, activation=\"nnelu\", name=\"sigmas\")(X)\n",
    "output = layers.Concatenate()([alphas, mus, sigmas])\n",
    "MDN_model = Model([input1, input2, input3], output)\n",
    "\n",
    "# define TaskExtractor\n",
    "\n",
    "taskextractor = TaskExtractor(data, lats_lons, task_dim, test_proportion, n_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(probdownscale.MetaTrain)\n",
    "from probdownscale.MetaTrain import MetaSGD\n",
    "# define meta learner\n",
    "meta_step = 20\n",
    "meta_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "inner_step = 1\n",
    "inner_optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "\n",
    "meta_learner = MetaSGD(MDN_model, gamma_loss, meta_step, meta_optimizer, inner_step, inner_optimizer, taskextractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 1 / 25 loss:  8.605626\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 2 / 25 loss:  8.884377\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 3 / 25 loss:  8.499913\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 4 / 25 loss:  7.8956017\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 5 / 25 loss:  8.310225\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 6 / 25 loss:  7.6332526\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 7 / 25 loss:  8.895291\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 8 / 25 loss:  10.236228\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 9 / 25 loss:  7.203607\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 10 / 25 loss:  7.3290353\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 11 / 25 loss:  7.0227633\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 12 / 25 loss:  8.168777\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 13 / 25 loss:  8.292377\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 14 / 25 loss:  6.875219\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 15 / 25 loss:  6.79422\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 16 / 25 loss:  6.691573\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 17 / 25 loss:  6.7326097\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 18 / 25 loss:  6.596742\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 19 / 25 loss:  6.556325\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 20 / 25 loss:  7.0924654\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 21 / 25 loss:  7.241005\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 22 / 25 loss:  6.4767838\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 23 / 25 loss:  6.390025\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 24 / 25 loss:  6.34634\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing basic training step: 25 / 25 loss:  7.0572977\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 1 / 20 loss:  7.0548477\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 2 / 20 loss:  7.221171\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 3 / 20 loss:  7.0412292\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 4 / 20 loss:  7.4290533\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 5 / 20 loss:  7.3310347\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 6 / 20 loss:  6.491572\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 7 / 20 loss:  6.9344735\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 8 / 20 loss:  7.3169546\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 9 / 20 loss:  7.362981\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 10 / 20 loss:  7.5335746\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 11 / 20 loss:  7.091314\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 12 / 20 loss:  6.8898406\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 13 / 20 loss:  7.051164\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 14 / 20 loss:  7.9486237\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 15 / 20 loss:  7.2642236\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 16 / 20 loss:  7.5306687\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 17 / 20 loss:  7.5332336\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 18 / 20 loss:  7.3351135\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 19 / 20 loss:  7.389099\n",
      "Meta lr: <tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n",
      "Now doing bootstrap training step: 20 / 20 loss:  7.2027817\n"
     ]
    }
   ],
   "source": [
    "meta_history = meta_learner.meta_fit(10, basic_train=True, bootstrap_train=True, use_test_for_meta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inner_rate_f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-be7e5f71bb24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mbatch_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdistance_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlocat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_locations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0minner_rate_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inner_rate_f' is not defined"
     ]
    }
   ],
   "source": [
    "def inner_rate_function(inner_rate, batch_size, inner_step):\n",
    "    return inner_rate/inner_step*math.log(batch_size, 20)\n",
    "\n",
    "def meta_rate_function(meta_rate, batch_locations, seen_locations, covariance_function, distance_function):\n",
    "    batch_size = len(batch_location)\n",
    "    center = np.average(list(seen_locations.keys()), weights=list(seen_locations.values()), axis=0)\n",
    "    batch_dist = np.mean([distance_function(locat, center) for locat in batch_locations])\n",
    "    \n",
    "inner_rate_f(0.01, 100, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Meta Training History')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(meta_history)\n",
    "plt.title('Meta Training History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_52\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_244 (InputLayer)         [(None, 10, 1, 3, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv_lstm2d_98 (ConvLSTM2D)    (None, 10, 1, 2, 20  3760        ['input_244[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_lstm2d_99 (ConvLSTM2D)    (None, 10, 1, 1, 20  6480        ['conv_lstm2d_98[1][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_lstm2d_100 (ConvLSTM2D)   (None, 1, 1, 20)     3280        ['conv_lstm2d_99[1][0]']         \n",
      "                                                                                                  \n",
      " flatten_104 (Flatten)          (None, 20)           0           ['conv_lstm2d_100[1][0]']        \n",
      "                                                                                                  \n",
      " dense_348 (Dense)              (None, 512)          10752       ['flatten_104[1][0]']            \n",
      "                                                                                                  \n",
      " input_245 (InputLayer)         [(None, 3, 3, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " input_246 (InputLayer)         [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_279 (Batch  (None, 512)         2048        ['dense_348[1][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 2, 2, 20)     100         ['input_245[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_280 (Batch  (None, 1)           4           ['input_246[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_349 (Dense)              (None, 300)          153900      ['batch_normalization_279[1][0]']\n",
      "                                                                                                  \n",
      " flatten_105 (Flatten)          (None, 80)           0           ['conv2d_52[1][0]']              \n",
      "                                                                                                  \n",
      " dense_350 (Dense)              (None, 30)           60          ['batch_normalization_280[1][0]']\n",
      "                                                                                                  \n",
      " concatenate_104 (Concatenate)  (None, 410)          0           ['dense_349[1][0]',              \n",
      "                                                                  'flatten_105[1][0]',            \n",
      "                                                                  'dense_350[1][0]']              \n",
      "                                                                                                  \n",
      " dense_351 (Dense)              (None, 128)          52608       ['concatenate_104[1][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_281 (Batch  (None, 128)         512         ['dense_351[1][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_352 (Dense)              (None, 128)          16512       ['batch_normalization_281[1][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_282 (Batch  (None, 128)         512         ['dense_352[1][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_353 (Dense)              (None, 128)          16512       ['batch_normalization_282[1][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_283 (Batch  (None, 128)         512         ['dense_353[1][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_354 (Dense)              (None, 45)           5805        ['batch_normalization_283[1][0]']\n",
      "                                                                                                  \n",
      " dense_355 (Dense)              (None, 45)           5805        ['batch_normalization_283[1][0]']\n",
      "                                                                                                  \n",
      " sigmas (Dense)                 (None, 45)           5805        ['batch_normalization_283[1][0]']\n",
      "                                                                                                  \n",
      " concatenate_105 (Concatenate)  (None, 135)          0           ['dense_354[1][0]',              \n",
      "                                                                  'dense_355[1][0]',              \n",
      "                                                                  'sigmas[1][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 284,967\n",
      "Trainable params: 283,173\n",
      "Non-trainable params: 1,794\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trained_meta_model = meta_learner.meta_model\n",
    "optimizer = tf.keras.optimizers.Adam(0.000001)\n",
    "trained_meta_model.compile(optimizer=optimizer, loss=gamma_loss)\n",
    "trained_meta_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 10s 90ms/step - loss: 5.1649 - val_loss: 5.3757\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.1973 - val_loss: 5.3607\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.1909 - val_loss: 5.3479\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.2363 - val_loss: 5.3394\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.2065 - val_loss: 5.3352\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.2603 - val_loss: 5.3316\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.2340 - val_loss: 5.3284\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.1792 - val_loss: 5.3248\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.2156 - val_loss: 5.3202\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 5.1938 - val_loss: 5.3210\n"
     ]
    }
   ],
   "source": [
    "meta_history_fine_tune = trained_meta_model.fit(train_x, train_y, epochs=10, validation_data=[test_x, test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Downscale_env",
   "language": "python",
   "name": "downscale_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
